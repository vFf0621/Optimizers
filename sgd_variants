#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 22 19:24:26 2020

@author: fguan
"""
from typing import Callable
import matplotlib.pyplot as plt
import torch

def func(x):
    return torch.sin(x)

def funcR3(x, y):
    return torch.sin(x * y)

def sgd(func: Callable, x: float, target: float, lr: float, iteration: int) ->\
        float:
    """ Standard gradient descent in R1.
    """
    lr = torch.tensor(lr, requires_grad=True)
    target = torch.tensor(target, requires_grad=True)
    for i in range(iteration):
        x = torch.tensor(x, requires_grad=True)
        evaluate = (target - func(x)) ** 2
        evaluate.backward()
        ddx = x.grad
        x = x - ddx * lr
        plt.plot(i, abs(ddx.item()), 'ro')
    return x.item()

def sgd_m(func: Callable, x: float, target: float, lr: float, iteration: int, 
        b: float=0.9) -> float:
    """ Gradient descent of R1 function with momentum.
    """
    b = torch.tensor(b, requires_grad=True)
    lr = torch.tensor(lr, requires_grad=True)
    target = torch.tensor(target, requires_grad=True)
    vx = torch.tensor(0.)
    for i in range(iteration):
        x = torch.tensor(x, requires_grad=True)
        evaluate = (target - func(x)) ** 2
        evaluate.backward()
        ddx = x.grad
        ddx = vx * b + (torch.tensor(1., requires_grad=True) - b) * ddx
        x = x - ddx * lr
        vx = ddx
        plt.plot(i, abs(ddx.item()), 'ro')
    return x.item()

def sgd_m_d(func: Callable, x: float, target: float, lr: float, iteration: int, 
       a: float=0.0001, b: float=0.9) -> float:
    """ Gradient descent of R1 function with momentum with lr decay.
    """
    a = torch.tensor(a)
    b = torch.tensor(b)
    lr = torch.tensor(lr, requires_grad=True)
    target = torch.tensor(target, requires_grad=True)
    vx = torch.tensor(0.)
    for i in range(iteration):
        x = torch.tensor(x, requires_grad=True)
        evaluate = (target - func(x)) ** 2
        evaluate.backward()
        ddx = x.grad
        ddx = vx * b + (torch.tensor(1., requires_grad=True) - b) * ddx
        lr = lr * torch.tensor(1.) / (torch.tensor(1.) + torch.tensor(i) * a)
        x = x - ddx * lr
        vx = ddx
        plt.plot(i, abs(ddx.item()), 'ro')
    return x.item()

def sgd_m__d_R3(funcR3: Callable, x: float, y: float, target: float, lr: float, 
    iteration: int, a: float=0.0001, b: float=0.9) -> float:
    """ Gradient descent in R3 space with momentum and lr decay.
    """
    lr = torch.tensor(lr, requires_grad=True)
    target = torch.tensor(target, requires_grad=True)
    vx = torch.tensor(0.)
    vy = torch.tensor(0.)
    for i in range(iteration):
        x = torch.tensor(x, requires_grad=True)
        y = torch.tensor(y, requires_grad=True)
        cost = (target - funcR3(x, y)) ** 2
        cost.backward(retain_graph=True)
        ddx = x.grad
        ddy = y.grad
        ddx = vx * b + (torch.tensor(1., requires_grad=True) - b) * ddx
        ddy = vy * b + (torch.tensor(1., requires_grad=True) - b) * ddy
        lr = lr * torch.tensor(1.) / (torch.tensor(1.) + torch.tensor(i) * a)
        x = x - lr * ddx
        y = y - lr * ddy
        plt.plot(i, abs(ddx.item()), "ro")
        plt.plot(i, abs(ddy.item()), "go")
        vx = ddx
        vy = ddy
    return x.item(), y.item()
